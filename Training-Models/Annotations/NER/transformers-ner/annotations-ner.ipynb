{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Train CryptoBert for NER"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "'E:\\\\Crypto\\\\Annotations\\\\NER'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.getcwd()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "[1, 2, 3, 0]"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer('vocab.txt')\n",
    "\n",
    "tokens = ['[UNK]', '[CLS]', '[SEP]', '[PAD]']\n",
    "\n",
    "special_tokens = [tokenizer.token_to_id(i) for i in tokens]\n",
    "special_tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from prodigy.components.loaders import JSONL\n",
    "\n",
    "data = JSONL('annotations.jsonl')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "row = {'input_ids': '', 'token_type_ids': '', 'attention_mask': '', 'labels': ''}\n",
    "\n",
    "for line in data:\n",
    "\n",
    "    if line['answer'] == 'accept':\n",
    "\n",
    "        try:\n",
    "            row['input_ids'] = [token['tokenizer_id'] for token in line['tokens']]\n",
    "            row['token_type_ids'] = [token['type_ids'] for token in line['tokens']]\n",
    "            row['attention_mask'] = [token['attention_mask'] for token in line['tokens']]\n",
    "            row['labels'] = [-100 if i['tokenizer_id'] in special_tokens else 0 for i in line['tokens']]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            for label in line['spans']:\n",
    "                start = int(label['token_start'])\n",
    "                end = int(label['token_end'])\n",
    "\n",
    "                row['labels'][start] = 1\n",
    "                for i in range(start, end):\n",
    "                    row['labels'][i+1] = 2\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    df = df.append(row, ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "assert len(df['input_ids']) == len(df['token_type_ids'])\n",
    "assert len(df['input_ids']) == len(df['attention_mask'])\n",
    "assert len(df['input_ids']) == len(df['labels'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "                                            input_ids  \\\n0   [2, 1436, 2105, 2924, 1407, 1436, 35, 7975, 13...   \n1   [2, 51, 11, 2489, 4611, 1436, 2174, 1436, 2255...   \n2   [2, 1436, 1489, 2511, 15679, 12937, 1379, 2987...   \n3   [2, 1814, 1713, 2949, 1381, 1370, 1956, 1385, ...   \n4   [2, 1436, 1529, 5828, 1370, 10681, 1555, 18, 1...   \n..                                                ...   \n95  [2, 2005, 1683, 1467, 1940, 1383, 2278, 1410, ...   \n96  [2, 1410, 283, 61, 1837, 1675, 1530, 1968, 181...   \n97  [2, 51, 1692, 1538, 1411, 1436, 16, 1599, 1483...   \n98  [2, 1380, 3283, 1849, 1998, 3513, 1380, 2005, ...   \n99  [2, 22197, 1021, 31, 3074, 2413, 2123, 5758, 7...   \n\n                                       token_type_ids  \\\n0                   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n1   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n2    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n3   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n4    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n..                                                ...   \n95  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n96  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n97  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n98  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n99  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n\n                                       attention_mask  \\\n0                   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n1   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n2    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n3   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n4    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n..                                                ...   \n95  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n96  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n97  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n98  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n99  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n\n                                               labels  \n0             [-100, 1, 0, 0, 0, 1, 0, 0, 0, 0, -100]  \n1   [-100, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...  \n2   [-100, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...  \n3   [-100, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...  \n4   [-100, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...  \n..                                                ...  \n95  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...  \n96  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...  \n97  [-100, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n98  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...  \n99  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n\n[100 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>input_ids</th>\n      <th>token_type_ids</th>\n      <th>attention_mask</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[2, 1436, 2105, 2924, 1407, 1436, 35, 7975, 13...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n      <td>[-100, 1, 0, 0, 0, 1, 0, 0, 0, 0, -100]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[2, 51, 11, 2489, 4611, 1436, 2174, 1436, 2255...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[-100, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[2, 1436, 1489, 2511, 15679, 12937, 1379, 2987...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n      <td>[-100, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[2, 1814, 1713, 2949, 1381, 1370, 1956, 1385, ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[2, 1436, 1529, 5828, 1370, 10681, 1555, 18, 1...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n      <td>[-100, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>[2, 2005, 1683, 1467, 1940, 1383, 2278, 1410, ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>[2, 1410, 283, 61, 1837, 1675, 1530, 1968, 181...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>[2, 51, 1692, 1538, 1411, 1436, 16, 1599, 1483...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[-100, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>[2, 1380, 3283, 1849, 1998, 3513, 1380, 2005, ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>[2, 22197, 1021, 31, 3074, 2413, 2123, 5758, 7...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows Ã— 4 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "features = datasets.Features(\n",
    "    {\n",
    "        'input_ids': datasets.Sequence(datasets.Value(\"int32\")),\n",
    "        'token_type_ids': datasets.Sequence(datasets.Value(\"int32\")),\n",
    "        'attention_mask': datasets.Sequence(datasets.Value(\"int32\")),\n",
    "        'labels': datasets.Sequence(datasets.ClassLabel(num_classes=3, names=['0', 'B-COIN', 'I-COIN'])),\n",
    "    }\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "training = df.iloc[:80,:]\n",
    "test = df.iloc[80:,:]\n",
    "\n",
    "dataset_train = Dataset.from_pandas(training, features=features)\n",
    "dataset_test = Dataset.from_pandas(test, features=features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "Sequence(feature=ClassLabel(num_classes=3, names=['0', 'B-COIN', 'I-COIN'], names_file=None, id=None), length=-1, id=None)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.features['labels']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BERT - NER Finetuning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('./CryptoBERT', padding=True, truncation=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]___________________________________ -100\n",
      "need____________________________________ 0\n",
      "fiat____________________________________ 0\n",
      "but_____________________________________ 0\n",
      "don_____________________________________ 0\n",
      "'_______________________________________ 0\n",
      "t_______________________________________ 0\n",
      "want____________________________________ 0\n",
      "to______________________________________ 0\n",
      "sell____________________________________ 0\n",
      "my______________________________________ 0\n",
      "bitcoin_________________________________ 1\n",
      "at______________________________________ 0\n",
      "any_____________________________________ 0\n",
      "cost____________________________________ 0\n",
      "is______________________________________ 0\n",
      "there___________________________________ 0\n",
      "a_______________________________________ 0\n",
      "way_____________________________________ 0\n",
      "to______________________________________ 0\n",
      "safely__________________________________ 0\n",
      "get_____________________________________ 0\n",
      "loans___________________________________ 0\n",
      "on______________________________________ 0\n",
      "bitcoin_________________________________ 1\n",
      "?_______________________________________ 0\n",
      "[SEP]___________________________________ -100\n"
     ]
    }
   ],
   "source": [
    "for token, label in zip(tokenizer.convert_ids_to_tokens(dataset_train[\"input_ids\"][20]),dataset_train[\"labels\"][20]):\n",
    "    print(f\"{token:_<40} {label}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./model were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at ./model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "model = AutoModelForTokenClassification.from_pretrained('./CryptoBERT', num_labels=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "args = TrainingArguments(\n",
    "    \"CryptoBERT-NER\",\n",
    "    no_cuda=True,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "import datasets\n",
    "import numpy as np\n",
    "\n",
    "metric = datasets.load_metric(\"seqeval\")\n",
    "\n",
    "label_list = ['0', 'B-COIN', 'I-COIN']\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    true_predictions = [[label_list[p] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n",
    "    true_labels = [[label_list[l] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions,\n",
    "                   references=true_labels)\n",
    "\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 80\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 15\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 2/15 : < :, Epoch 0.20/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 16\n",
      "E:\\Crypto\\winvenv\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: 0 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "TrainOutput(global_step=15, training_loss=0.13428414662679036, metrics={'train_runtime': 138.726, 'train_samples_per_second': 1.73, 'train_steps_per_second': 0.108, 'total_flos': 19083001292064.0, 'train_loss': 0.13428414662679036, 'epoch': 3.0})"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_test,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in nermodel\\config.json\n",
      "Model weights saved in nermodel\\pytorch_model.bin\n",
      "tokenizer config file saved in tokenizer\\tokenizer_config.json\n",
      "Special tokens file saved in tokenizer\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": "('tokenizer\\\\tokenizer_config.json',\n 'tokenizer\\\\special_tokens_map.json',\n 'tokenizer\\\\vocab.txt',\n 'tokenizer\\\\added_tokens.json',\n 'tokenizer\\\\tokenizer.json')"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"CryptoBERT-NER\")\n",
    "tokenizer.save_pretrained(\"tokenizer\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    str(i): label for i,label in enumerate(label_list)\n",
    "}\n",
    "label2id = {\n",
    "    label: str(i) for i,label in enumerate(label_list)\n",
    "}\n",
    "import json\n",
    "config = json.load(open(\"CryptoBERT-NER/config.json\"))\n",
    "config[\"id2label\"] = id2label\n",
    "config[\"label2id\"] = label2id\n",
    "json.dump(config, open(\"CryptoBERT-NER/config.json\",\"w\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file nermodel\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"nermodel\",\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"0\",\n",
      "    \"1\": \"B-COIN\",\n",
      "    \"2\": \"I-COIN\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"0\": \"0\",\n",
      "    \"B-COIN\": \"1\",\n",
      "    \"I-COIN\": \"2\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file nermodel\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForTokenClassification.\n",
      "\n",
      "All the weights of BertForTokenClassification were initialized from the model checkpoint at nermodel.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForTokenClassification for predictions without further training.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': '0', 'score': 0.99573696, 'index': 1, 'word': 'i', 'start': 0, 'end': 1}, {'entity': '0', 'score': 0.9975593, 'index': 2, 'word': 'think', 'start': 2, 'end': 7}, {'entity': 'B-COIN', 'score': 0.98610383, 'index': 3, 'word': 'bitcoin', 'start': 8, 'end': 15}, {'entity': '0', 'score': 0.99839956, 'index': 4, 'word': 'is', 'start': 16, 'end': 18}, {'entity': '0', 'score': 0.9960913, 'index': 5, 'word': 'a', 'start': 19, 'end': 20}, {'entity': '0', 'score': 0.997276, 'index': 6, 'word': 'good', 'start': 21, 'end': 25}, {'entity': '0', 'score': 0.9943117, 'index': 7, 'word': 'investment', 'start': 26, 'end': 36}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"CryptoBERT-NER\")\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': '0', 'score': 0.9968875, 'index': 1, 'word': 'i', 'start': 0, 'end': 1}, {'entity': '0', 'score': 0.9980868, 'index': 2, 'word': 'think', 'start': 2, 'end': 7}, {'entity': 'B-COIN', 'score': 0.9888265, 'index': 3, 'word': 'bitcoin', 'start': 8, 'end': 15}, {'entity': '0', 'score': 0.99873, 'index': 4, 'word': 'is', 'start': 16, 'end': 18}, {'entity': '0', 'score': 0.99634784, 'index': 5, 'word': 'a', 'start': 19, 'end': 20}, {'entity': '0', 'score': 0.9978637, 'index': 6, 'word': 'good', 'start': 21, 'end': 25}, {'entity': '0', 'score': 0.9955788, 'index': 7, 'word': 'investment', 'start': 26, 'end': 36}, {'entity': '0', 'score': 0.9977976, 'index': 8, 'word': '.', 'start': 36, 'end': 37}, {'entity': 'B-COIN', 'score': 0.9850418, 'index': 9, 'word': 'bitcoin', 'start': 38, 'end': 45}, {'entity': '0', 'score': 0.99808645, 'index': 10, 'word': 'can', 'start': 46, 'end': 49}, {'entity': '0', 'score': 0.9990214, 'index': 11, 'word': 'save', 'start': 50, 'end': 54}, {'entity': '0', 'score': 0.9959998, 'index': 12, 'word': 'the', 'start': 55, 'end': 58}, {'entity': '0', 'score': 0.996014, 'index': 13, 'word': 'world', 'start': 59, 'end': 64}]\n"
     ]
    }
   ],
   "source": [
    "example = \"I think bitcoin is a good investment. bitcoin can save the world\"\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-COIN', 'score': 0.9825546, 'index': 1, 'word': 'bitcoin', 'start': 0, 'end': 7}, {'entity': 'B-COIN', 'score': 0.98775333, 'index': 2, 'word': 'bitcoin', 'start': 8, 'end': 15}, {'entity': '0', 'score': 0.99839133, 'index': 3, 'word': '.', 'start': 15, 'end': 16}, {'entity': 'B-COIN', 'score': 0.98801965, 'index': 4, 'word': 'bitcoin', 'start': 17, 'end': 24}, {'entity': '0', 'score': 0.9980869, 'index': 5, 'word': 'can', 'start': 25, 'end': 28}, {'entity': '0', 'score': 0.99900657, 'index': 6, 'word': 'save', 'start': 29, 'end': 33}, {'entity': '0', 'score': 0.99587846, 'index': 7, 'word': 'the', 'start': 34, 'end': 37}, {'entity': '0', 'score': 0.9953231, 'index': 8, 'word': 'world', 'start': 38, 'end': 43}]\n"
     ]
    }
   ],
   "source": [
    "example = \"bitcoin bitcoin. bitcoin can save the world\"\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}