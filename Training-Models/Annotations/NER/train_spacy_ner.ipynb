{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "from prodigy.components.loaders import JSONL\n",
    "\n",
    "data = JSONL('annotated_data.jsonl')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Bitcoin bull bullish on bitcoin? Shocker.', {'entities': [(0, 7, 'COIN'), (24, 31, 'COIN')]}), (\"I've owned bitcoin before Bitcoin Cash came out, do I own bitcoin cash in some way now?\", {'entities': [(11, 18, 'COIN'), (26, 33, 'COIN'), (58, 65, 'COIN')]})]\n"
     ]
    }
   ],
   "source": [
    "TRAINING_DATA = []\n",
    "\n",
    "for line in data:\n",
    "    try:\n",
    "        if line['answer'] == 'accept':\n",
    "            labels = []\n",
    "            for span in line['spans']:\n",
    "                labels.append((span['start'], span['end'], span['label']))\n",
    "\n",
    "            TRAINING_DATA.append(\n",
    "                (line['text'], {'entities': labels})\n",
    "            )\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(TRAINING_DATA[:2])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Spacy 2.x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# !python -m spacy download en_core_web_md\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "ner = nlp.get_pipe('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "LABEL = 'COIN'\n",
    "\n",
    "# Add the new label to ner\n",
    "ner.add_label(LABEL)\n",
    "\n",
    "# Resume training\n",
    "optimizer = nlp.resume_training()\n",
    "move_names = list(ner.move_names)\n",
    "\n",
    "# List of pipes you want to train\n",
    "pipe_exceptions = [\"ner\"]\n",
    "\n",
    "# List of pipes which should remain unaffected in training\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 3.2809867044565006}\n",
      "Losses {'ner': 17.148286672739545}\n",
      "Losses {'ner': 19.143903895659488}\n",
      "Losses {'ner': 23.31138708494987}\n",
      "Losses {'ner': 24.982092594455338}\n",
      "Losses {'ner': 26.193015469181525}\n",
      "Losses {'ner': 29.375631557283}\n",
      "Losses {'ner': 35.31397430056788}\n",
      "Losses {'ner': 36.61637848970324}\n",
      "Losses {'ner': 53.31381365031588}\n",
      "Losses {'ner': 56.42848939680457}\n",
      "Losses {'ner': 62.770804703857436}\n",
      "Losses {'ner': 64.61610381812974}\n",
      "Losses {'ner': 66.65294904622947}\n",
      "Losses {'ner': 69.63741609304988}\n",
      "Losses {'ner': 80.9910823803671}\n",
      "Losses {'ner': 97.02652371437436}\n",
      "Losses {'ner': 110.7332091786427}\n",
      "Losses {'ner': 111.91463260771137}\n",
      "Losses {'ner': 115.19788737811591}\n",
      "Losses {'ner': 117.7527480469925}\n",
      "Losses {'ner': 128.64379310646694}\n",
      "Losses {'ner': 131.49994478887206}\n",
      "Losses {'ner': 138.6326529527616}\n",
      "Losses {'ner': 140.8014868369341}\n",
      "Losses {'ner': 141.89273207906461}\n",
      "Losses {'ner': 144.0253156260464}\n",
      "Losses {'ner': 145.69008638920656}\n",
      "Losses {'ner': 146.6561745490866}\n",
      "Losses {'ner': 149.5638265316855}\n",
      "Losses {'ner': 150.21521119169978}\n",
      "Losses {'ner': 151.8920560379765}\n",
      "Losses {'ner': 155.46607593644654}\n",
      "Losses {'ner': 155.77646886880217}\n",
      "Losses {'ner': 158.3867809610472}\n",
      "Losses {'ner': 159.04140813890902}\n",
      "Losses {'ner': 159.58303884170647}\n",
      "Losses {'ner': 161.59401725213624}\n",
      "Losses {'ner': 161.81453966306285}\n",
      "Losses {'ner': 162.68933064477093}\n",
      "Losses {'ner': 163.86411129289988}\n",
      "Losses {'ner': 164.76114482856593}\n",
      "Losses {'ner': 164.7939119815598}\n",
      "Losses {'ner': 164.8791671783343}\n",
      "Losses {'ner': 164.93843664483418}\n",
      "Losses {'ner': 165.02533541627503}\n",
      "Losses {'ner': 165.08422537033476}\n",
      "Losses {'ner': 166.13957187445092}\n",
      "Losses {'ner': 168.2304253997106}\n",
      "Losses {'ner': 168.3056757382254}\n",
      "Losses {'ner': 168.66960699520342}\n",
      "Losses {'ner': 168.6726868125079}\n",
      "Losses {'ner': 168.71904752375153}\n",
      "Losses {'ner': 168.7202504139607}\n",
      "Losses {'ner': 168.72067500604103}\n",
      "Losses {'ner': 168.83110337814503}\n",
      "Losses {'ner': 170.95611438924306}\n",
      "Losses {'ner': 172.33895447468373}\n",
      "Losses {'ner': 179.35845672635799}\n",
      "Losses {'ner': 180.35184848210076}\n",
      "Losses {'ner': 180.35860990351304}\n",
      "Losses {'ner': 180.40520731076182}\n",
      "Losses {'ner': 180.40522468928276}\n",
      "Losses {'ner': 180.4053323088908}\n",
      "Losses {'ner': 180.40541984199888}\n",
      "Losses {'ner': 180.40558379084925}\n",
      "Losses {'ner': 180.48618567925118}\n",
      "Losses {'ner': 180.48619513474137}\n",
      "Losses {'ner': 180.4864531822162}\n",
      "Losses {'ner': 180.79315946929367}\n",
      "Losses {'ner': 180.84348891263616}\n",
      "Losses {'ner': 180.84348908056202}\n",
      "Losses {'ner': 180.8434894542482}\n",
      "Losses {'ner': 180.84348946472306}\n",
      "Losses {'ner': 180.84348946635478}\n",
      "Losses {'ner': 180.9264383648487}\n",
      "Losses {'ner': 180.92643866288918}\n",
      "Losses {'ner': 180.92681260097768}\n",
      "Losses {'ner': 180.92849276553747}\n",
      "Losses {'ner': 180.92849276919168}\n",
      "Losses {'ner': 180.92928444801427}\n",
      "Losses {'ner': 181.10295115274928}\n",
      "Losses {'ner': 181.10295176024957}\n",
      "Losses {'ner': 181.10295190669095}\n",
      "Losses {'ner': 181.1029840596154}\n",
      "Losses {'ner': 181.1034471762804}\n",
      "Losses {'ner': 181.10346538100043}\n",
      "Losses {'ner': 181.1034695012489}\n",
      "Losses {'ner': 181.1034695026478}\n",
      "Losses {'ner': 181.10346951645087}\n",
      "Losses {'ner': 181.10346995762987}\n",
      "Losses {'ner': 181.1034699743509}\n",
      "Losses {'ner': 181.10347053619364}\n",
      "Losses {'ner': 181.103470578612}\n",
      "Losses {'ner': 181.10347057901706}\n",
      "Losses {'ner': 181.10349059620015}\n",
      "Losses {'ner': 181.10349073421452}\n",
      "Losses {'ner': 181.10349094744362}\n"
     ]
    }
   ],
   "source": [
    "from spacy.training.example import Example\n",
    "import random\n",
    "\n",
    "# Begin training by disabling other pipeline components\n",
    "with nlp.disable_pipes(*other_pipes):\n",
    "    random.shuffle(TRAINING_DATA)\n",
    "    losses = {}\n",
    "    for batch in spacy.util.minibatch(TRAINING_DATA, size=40):\n",
    "        for text, annotations in batch:\n",
    "            # create Example\n",
    "            doc = nlp.make_doc(text)\n",
    "            example = Example.from_dict(doc, annotations)\n",
    "            # Update the model\n",
    "            nlp.update([example], losses=losses, drop=0.3)\n",
    "            print(\"Losses\", losses)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities in  I want to buy bitcoin, usd and solana\n",
      "bitcoin\n"
     ]
    }
   ],
   "source": [
    "test_text = \"I want to buy bitcoin, usd and solana\"\n",
    "doc = nlp(test_text)\n",
    "print(\"Entities in \", test_text)\n",
    "for ent in doc.ents:\n",
    "    print(ent)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Spacy 3.0 - Prodigy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Using CPU\n",
      "\u001B[1m\n",
      "========================= Generating Prodigy config =========================\u001B[0m\n",
      "[i] Auto-generating config with spaCy\n",
      "[+] Generated training config\n",
      "\u001B[1m\n",
      "=========================== Initializing pipeline ===========================\u001B[0m\n",
      "[+] Initialized pipeline\n",
      "\u001B[1m\n",
      "============================= Training pipeline =============================\u001B[0m\n",
      "[i] Pipeline: ['tok2vec', 'ner']\n",
      "[i] Initial learn rate: 0.001\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00     33.33    1.70    1.49    1.97    0.02\n",
      "  4     200         59.98   1305.01   95.68   96.64   94.74    0.96\n",
      "  9     400         87.70    139.41   94.59   97.22   92.11    0.95\n",
      " 15     600         84.12     71.04   95.74   95.42   96.05    0.96\n",
      " 22     800         69.48     40.42   95.65   97.28   94.08    0.96\n",
      " 32    1000         70.42     44.52   95.33   96.62   94.08    0.95\n",
      " 43    1200         85.51     25.79   94.70   95.33   94.08    0.95\n",
      " 57    1400         53.93     12.82   94.74   94.74   94.74    0.95\n",
      " 75    1600         11.58      2.01   95.05   95.36   94.74    0.95\n",
      " 96    1800        213.46     37.03   95.71   96.03   95.39    0.96\n",
      "122    2000        178.72     30.63   95.39   95.39   95.39    0.95\n",
      "154    2200        210.23     36.94   94.08   94.08   94.08    0.94\n",
      "[+] Saved pipeline to output directory\n",
      "model\\model-last\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-01-18 11:23:20,785] [INFO] Set up nlp object from config\n",
      "Components: ner\n",
      "Merging training and evaluation data for 1 components\n",
      "  - [ner] Training: 413 | Evaluation: 103 (20% split)\n",
      "Training: 351 | Evaluation: 95\n",
      "Labels: ner (1)\n",
      "[2022-01-18 11:23:20,874] [INFO] Pipeline: ['tok2vec', 'ner']\n",
      "[2022-01-18 11:23:20,878] [INFO] Created vocabulary\n",
      "[2022-01-18 11:23:20,879] [INFO] Finished initializing nlp object\n",
      "[2022-01-18 11:23:21,190] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
      "Components: ner\n",
      "Merging training and evaluation data for 1 components\n",
      "  - [ner] Training: 413 | Evaluation: 103 (20% split)\n",
      "Training: 351 | Evaluation: 95\n",
      "Labels: ner (1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Using CPU\n",
      "\u001B[1m\n",
      "========================= Generating Prodigy config =========================\u001B[0m\n",
      "[i] Auto-generating config with spaCy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-01-18 11:25:55,310] [INFO] Set up nlp object from config\n",
      "Components: ner\n",
      "Merging training and evaluation data for 1 components\n",
      "  - [ner] Training: 413 | Evaluation: 103 (20% split)\n",
      "Training: 351 | Evaluation: 95\n",
      "Labels: ner (1)\n",
      "[2022-01-18 11:25:55,384] [INFO] Pipeline: ['tok2vec', 'ner']\n",
      "[2022-01-18 11:25:55,388] [INFO] Created vocabulary\n",
      "[2022-01-18 11:25:55,389] [INFO] Finished initializing nlp object\n",
      "[2022-01-18 11:25:55,748] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
      "Components: ner\n",
      "Merging training and evaluation data for 1 components\n",
      "  - [ner] Training: 413 | Evaluation: 103 (20% split)\n",
      "Training: 351 | Evaluation: 95\n",
      "Labels: ner (1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Generated training config\n",
      "\u001B[1m\n",
      "=========================== Initializing pipeline ===========================\u001B[0m\n",
      "[+] Initialized pipeline\n",
      "\u001B[1m\n",
      "============================= Training pipeline =============================\u001B[0m\n",
      "[i] Pipeline: ['tok2vec', 'ner']\n",
      "[i] Initial learn rate: 0.001\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00     33.33    1.70    1.49    1.97    0.02\n",
      "  4     200         59.98   1305.01   95.68   96.64   94.74    0.96\n",
      "  9     400         87.70    139.41   94.59   97.22   92.11    0.95\n",
      " 15     600         84.12     71.04   95.74   95.42   96.05    0.96\n",
      " 22     800         69.48     40.42   95.65   97.28   94.08    0.96\n",
      " 32    1000         70.42     44.52   95.33   96.62   94.08    0.95\n",
      " 43    1200         85.51     25.79   94.70   95.33   94.08    0.95\n",
      " 57    1400         53.93     12.82   94.74   94.74   94.74    0.95\n",
      " 75    1600         11.58      2.01   95.05   95.36   94.74    0.95\n",
      " 96    1800        213.46     37.03   95.71   96.03   95.39    0.96\n",
      "122    2000        178.72     30.63   95.39   95.39   95.39    0.95\n",
      "154    2200        210.23     36.94   94.08   94.08   94.08    0.94\n",
      "[+] Saved pipeline to output directory\n",
      "model\\model-last\n"
     ]
    }
   ],
   "source": [
    "!python -m prodigy train ./model --ner ner_cryptos"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('./model/model-best')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities in  I want to buy bitcoin, usd and solana\n",
      "bitcoin\n",
      "solana\n"
     ]
    }
   ],
   "source": [
    "test_text = \"I want to buy bitcoin, usd and solana\"\n",
    "doc = nlp(test_text)\n",
    "print(\"Entities in \", test_text)\n",
    "for ent in doc.ents:\n",
    "    print(ent)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}